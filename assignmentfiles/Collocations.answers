# How complete is your program?

My program is complete. Developed a class named NLPCollocations which contains all
the modules necessary to perform tokenization, stopwords removal (I remove stop words)
as I believe that stopwords should not be included in collocations, e.g. I am might
occur many times together but it should not be a collocation.

Given that, I tried 2 methods for chi-square. One mentioned in PPT and other in book.
I did this because the code was returning large values for chi-square and I felt it wrong.
Regardless, both the methods return same result. One way I found the large value problem
might be avoided is by using (Oi - Ei)**2 / sum(E) instead of (Oi - Ei)**2 / Ei

Afterwards, I reverse sorted the resultant based on value(2nd key) in tuple ((word1,word2),value).


#  top 20 bigrams along with their chi-square scores, in reverse
order of the scores

Top 20 Reverse Sorted based on chi-square
[(('Wickes', 'Horsehead'), 336973.9999970324), (('Golf', 'Course'), 336973.9999970324), (('Traveling', 'Abroad'), 336973.9999970324), (('Mill', 'Playhouse'), 336973.9999970324), (('Perfect', 'Witness'), 336973.9999970324), (('Witness', 'Aidan'), 336973.9999970324), (('Dennehy', 'Stockard'), 336973.9999970324), (('Stockard', 'Channing'), 336973.9999970324), (('Final', 'Days'), 336973.9999970324), (('Brideshead', 'Revisited'), 336973.9999970324), (('schizoid', 'horror'), 336973.9999970324), (('defiance', 'modernist'), 336973.9999970324), (('Performances', 'kicks'), 336973.9999970324), (('Polished', 'hooves'), 336973.9999970324), (('Shiny', 'Nikes'), 336973.9999970324), (('Glory', 'Enough'), 336973.9999970324), (('Ku', 'Klux'), 336973.9999970324), (('Klux', 'Klan'), 336973.9999970324), (('Latest', 'Period'), 336973.9999970324), (('Period', 'Sharpest'), 336973.9999970324)]


# top 20 bigrams along with their PMI scores, in reverse order of
the scores

Top 20 Reverse Sorted based on PMI
[(('Wickes', 'Horsehead'), 12.727764022615277), (('Golf', 'Course'), 12.727764022615277), (('Traveling', 'Abroad'), 12.727764022615277), (('Mill', 'Playhouse'), 12.727764022615277), (('Perfect', 'Witness'), 12.727764022615277), (('Witness', 'Aidan'), 12.727764022615277), (('Dennehy', 'Stockard'), 12.727764022615277), (('Stockard', 'Channing'), 12.727764022615277), (('Final', 'Days'), 12.727764022615277), (('Brideshead', 'Revisited'), 12.727764022615277), (('schizoid', 'horror'), 12.727764022615277), (('defiance', 'modernist'), 12.727764022615277), (('Performances', 'kicks'), 12.727764022615277), (('Polished', 'hooves'), 12.727764022615277), (('Shiny', 'Nikes'), 12.727764022615277), (('Glory', 'Enough'), 12.727764022615277), (('Ku', 'Klux'), 12.727764022615277), (('Klux', 'Klan'), 12.727764022615277), (('Latest', 'Period'), 12.727764022615277), (('Period', 'Sharpest'), 12.727764022615277)]

# A brief discussion of which of the two measures you believe works better to identify
collocations, based on your analysis of the top 20 bigrams produced by each measure.

At first I thought something might be wrong because top 20 were almost 
similar due to same resulting value (12.727 for pmi and 336973.999 chi-square) then 
I tried to do research about it and stumbled across this link:
https://stats.stackexchange.com/questions/125660/chi-squared-vs-mutual-information which says
that it should return almost similar results which conforms to my results.

Having said that, I think implementation wise PMI is better and result wise chi-square might be
better as it takes into account features for all cases. (both present,w1 present w2 not, w1 not, w2 present
and both not). But personally I would choose PMI over chi-square due to implementation simplicity and 
computational efficiency. It saves cost for both human hours and computational time as well.

NOTE: I changed name from collocations.py to Collocations.py after taking screenshot.
