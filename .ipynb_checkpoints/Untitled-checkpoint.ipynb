{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import multiprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsFeature:\n",
    "    def __init__(self, df, label_encoder):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.label_encoder = label_encoder\n",
    "        # self.train_le()\n",
    "\n",
    "    def get_next_word(self, index, orignal=False):\n",
    "        \"\"\"\n",
    "        Given an index of a word, returns the next word from the dataframe\n",
    "        params:\n",
    "            index: int\n",
    "        returns:\n",
    "            word: str\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if orignal:\n",
    "                return self.df.iloc[index + 1][1]\n",
    "            return self.label_encoder.transform([self.df.iloc[index + 1][1]])[0]\n",
    "        except IndexError:\n",
    "            if orignal:\n",
    "                return \"<END>\"\n",
    "            return self.label_encoder.transform([\"<END>\"])[0]\n",
    "        except ValueError:\n",
    "            # Returning -1 for unseen words\n",
    "            return -1\n",
    "\n",
    "    def get_prev_word(self, index, orignal=False):\n",
    "        \"\"\"\n",
    "        Given an index of a word, returns the word before '.' from the dataframe\n",
    "        params:\n",
    "            index: int\n",
    "        returns:\n",
    "            word: str\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            word = self.df.iloc[index][1]\n",
    "            if word[-1] == \".\":\n",
    "                if orignal:\n",
    "                    return word[:-1]\n",
    "                return self.label_encoder.transform([word[:-1]])[0]\n",
    "            else:\n",
    "                # NOT A PERIOD\n",
    "                # I think it would be better to return a <NAP> token\n",
    "                # This might also help in cleaning the data\n",
    "                # If orignal is true return word as is...\n",
    "                if orignal:\n",
    "                    return word\n",
    "#                 return self.label_encoder.transform([word])[0]\n",
    "                return self.label_encoder.transform([\"<NAP>\"])[0]\n",
    "        except ValueError:\n",
    "            # Returning -1 for unseen words\n",
    "            return -1\n",
    "        except IndexError:\n",
    "            if orignal:\n",
    "                return \"<START>\"\n",
    "            return self.label_encoder.transform([\"<START>\"])[0]\n",
    "\n",
    "    def lt_3(self, index):\n",
    "        \"\"\"\n",
    "        Given an index of a word, returns True if length of word before '.' is < 3 in the dataframe\n",
    "        params:\n",
    "            index: int\n",
    "        returns:\n",
    "            word: str\n",
    "        \"\"\"\n",
    "        word = self.get_prev_word(index, orignal=True)\n",
    "        return len(word) < 3\n",
    "\n",
    "    def is_cap_word(self, word):\n",
    "        \"\"\"\n",
    "        Given an index of a word, returns True if the word is capitalized in the dataframe\n",
    "        params:\n",
    "            index: int (index of word)\n",
    "            i: int\n",
    "        returns:\n",
    "            is_capital: bool\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return word[0].isupper()\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def actual_len(self, index):\n",
    "        \"\"\"\n",
    "        Returns the actual length of the previous word\n",
    "        \"\"\"\n",
    "        word = self.get_prev_word(index, orignal=True)\n",
    "        return len(word)\n",
    "    \n",
    "    def gt_x(self, index,x=4):\n",
    "        \"\"\"\n",
    "        Given an index of a word, returns True if length of word before '.' is < 3 in the dataframe\n",
    "        params:\n",
    "            index: int\n",
    "        returns:\n",
    "            word: str\n",
    "        \"\"\"\n",
    "        word = self.get_prev_word(index, orignal=True)\n",
    "        return len(word) > x\n",
    "\n",
    "    def get_average_len(self, index):\n",
    "        \"\"\"\n",
    "        Returns the average length of the previous word and next word\n",
    "        \"\"\"\n",
    "        prev_word = self.get_prev_word(index, orignal=True)\n",
    "        next_word = self.get_next_word(index, orignal=True)\n",
    "        return (len(prev_word) + len(next_word)) / 2\n",
    "\n",
    "    def get_avg_len(self, index, window=4):\n",
    "        \"\"\"\n",
    "        Given a window, returns the average length of the words in the window\n",
    "        (before current word only)\n",
    "\n",
    "        params:\n",
    "            index: int\n",
    "            window: int (default 4)\n",
    "        returns:\n",
    "            avg_len: float\n",
    "        \"\"\"\n",
    "        if index < 4:\n",
    "            words = [len(self.get_prev_word(i, orignal=True)) for i in range(1, index)]\n",
    "        else:\n",
    "            words = [\n",
    "                len(self.get_prev_word(index - i, orignal=True)) for i in range(window)\n",
    "            ]\n",
    "        try:\n",
    "            return sum(words) / len(words)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def get_feature(self, index, nof=5):\n",
    "        \"\"\"\n",
    "        Given an index, return corresponding features\n",
    "        params:\n",
    "            index: int\n",
    "        returns:\n",
    "            feature: list (prev word, next word, left_word<3, left_is_cap, right_is_cap, label)\n",
    "        \"\"\"\n",
    "        if index % 1000 == 0:\n",
    "            print(index)\n",
    "        feature = []\n",
    "        # word to the left\n",
    "        if nof > 0:\n",
    "            feature.append(self.get_prev_word(index))\n",
    "        # word to the right\n",
    "        if nof > 1:\n",
    "            feature.append(self.get_next_word(index))\n",
    "        # length of word to the left > 3\n",
    "        if nof > 2:\n",
    "            feature.append(self.lt_3(index))\n",
    "        # Left word is capitalized\n",
    "        if nof > 3:\n",
    "            feature.append(self.is_cap_word(self.get_prev_word(index, orignal=True)))\n",
    "        # Right word is capitalized\n",
    "        if nof > 4:\n",
    "            feature.append(self.is_cap_word(self.get_next_word(index, orignal=True)))\n",
    "\n",
    "        # More three features\n",
    "        if nof > 5:\n",
    "#             feature.append(self.gt_x(index,x=4))\n",
    "#             feature.append(self.gt_x(index,x=5))\n",
    "#             feature.append(self.gt_x(index,x=6))\n",
    "#             feature.append(self.gt_x(index,x=7))\n",
    "            feature.append(self.get_avg_len(index))\n",
    "        if nof > 6:\n",
    "            feature.append(self.get_average_len(index))\n",
    "        if nof > 7:\n",
    "            feature.append(self.actual_len(index))\n",
    "\n",
    "        # Finally add label token\n",
    "        feature.append(self.df[2][index])\n",
    "        return feature\n",
    "\n",
    "    def train_le(self):\n",
    "        \"\"\"\n",
    "        It trains label encoder with the appropriate data.\n",
    "        \"\"\"\n",
    "\n",
    "        lisa = [self.get_prev_word(i, orignal=True) for i in range(len(self.df))]\n",
    "        lisb = [self.get_next_word(i, orignal=True) for i in range(len(self.df))]\n",
    "        lis = lisa + lisb\n",
    "        lis.append(\"<NAP>\")\n",
    "        lis.append(\"<START>\")\n",
    "        return self.label_encoder.fit(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarvesh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37000\n",
      "20000\n",
      "45000\n",
      "12000\n",
      "70000\n",
      "148000\n",
      "131000\n",
      "209000\n",
      "173000\n",
      "193000\n",
      "160000\n",
      "200000\n",
      "263000\n",
      "266000\n",
      "278000\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Sentence Boundary Detection\")\n",
    "# parser.add_argument(\"train\", help=\"Training file\")\n",
    "# parser.add_argument(\"test\", help=\"Test file\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "train_file = \"SBD.train\"\n",
    "test_file = \"SBD.test\"\n",
    "# train_file = args.train\n",
    "# test_file = args.test\n",
    "\n",
    "# Check if train file and test file exists\n",
    "# if not os.path.isfile(train_file) or os.path.isfile(test_file):\n",
    "#     print(\"Training or Testing file does not exist\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "# Read the training file\n",
    "f = pd.read_csv(train_file, sep=r\"\\s\", header=None)\n",
    "f = f.drop(0, axis=1)\n",
    "\n",
    "# Resultant file\n",
    "#        1    2\n",
    "# 0     On  TOK\n",
    "# 1   June  TOK\n",
    "# 2      4  TOK\n",
    "# 3      ,  TOK\n",
    "# 4  after  TOK\n",
    "\n",
    "wf = WordsFeature(f, label_encoder=LabelEncoder())\n",
    "le = wf.train_le()\n",
    "nf = f[1].str.extract(r'([a-zA-Z0-9\"]+)([.]+)')\n",
    "\n",
    "nf.dropna(inplace=True)\n",
    "\n",
    "ilst = list(nf[0].index)\n",
    "pool = multiprocessing.Pool()\n",
    "pool = multiprocessing.Pool(processes=4)\n",
    "lst = pool.map(wf.get_feature, ilst)\n",
    "pool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have list containing features. We will now convert this to pandas dataframe.\n",
    "\n",
    "# Two methods to move forward, either simply discard TOK and move with just EOS and NEOS\n",
    "# or we can keep TOK and move with EOS and NEOS\n",
    "# We will use the second method, if it works then good else we will try the first method.\n",
    "\n",
    "# Try 1\n",
    "# keep features only\n",
    "x = [lst[i][:-1] for i in range(len(lst))]\n",
    "y = [lst[i][-1] for i in range(len(lst))]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x, y)\n",
    "features = 5\n",
    "# We can save dtree as pickle...\n",
    "with open(\"dtree{}.pkl\".format(features), \"wb\") as dfile:\n",
    "    pickle.dump(clf, dfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarvesh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000\n",
      "51000\n",
      "46000\n",
      "91000\n",
      "139000\n",
      "140000\n",
      "143000\n",
      "116000\n",
      "0.8721399730820996\n"
     ]
    }
   ],
   "source": [
    "# Test phase\n",
    "\n",
    "td = pd.read_csv(test_file, sep=r\"\\s\", header=None)\n",
    "td = td.drop(0, axis=1)\n",
    "twf = WordsFeature(td, le)\n",
    "tnf = td[1].str.extract(r'([a-zA-Z0-9\"]+)([.]+)')\n",
    "tnf.dropna(inplace=True)\n",
    "tilst = list(tnf.index)\n",
    "tpool = multiprocessing.Pool()\n",
    "tpool = multiprocessing.Pool(processes=4)\n",
    "tlst = tpool.map(twf.get_feature, tilst)\n",
    "tpool.close()\n",
    "\n",
    "# Splitting into training and testing data...\n",
    "xt = [tlst[i][:-1] for i in range(len(tlst))]\n",
    "yt = [tlst[i][-1] for i in range(len(tlst))]\n",
    "\n",
    "preds = clf.predict(xt)\n",
    "\n",
    "print(accuracy_score(yt, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 features: 87.81%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More features: 87.21% Actually reduces accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed NAP tokens... All features... I think this might increase accuracy as it will become aware of the word.\n",
    "# Wrong hypothesis.\n",
    "# It actually increases the accuracy for some reason?...\n",
    "# I think label encoder is causing problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using get_avg_len and gt_x with x=6\n",
    "# It might increase the accuracy\n",
    "# Also I think the accuracy is reduced due to 3 class classification instead of 2...\n",
    "# 87.31%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda99bcb4b0a0be4110a87e42072b64ae25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
